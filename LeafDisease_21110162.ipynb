{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49859ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2821d711",
   "metadata": {},
   "source": [
    "# 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11174fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = ['vision/training_set/burn_disease',\n",
    "           'vision/training_set/healthy',\n",
    "           'vision/training_set/leafspot']\n",
    "\n",
    "TEST_DIR = ['vision/test_set/burn_disease',\n",
    "           'vision/test_set/healthy',\n",
    "           'vision/test_set/leafspot']\n",
    "\n",
    "MODEL_DIR = 'save_model'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a3493fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train list: 1284\n",
      "test list: 15\n"
     ]
    }
   ],
   "source": [
    "# train image load\n",
    "train_list = []\n",
    "for i in DATA_DIR:\n",
    "    for _, file in enumerate(sorted(glob(i + '/*'))):\n",
    "        train_list.append(file)\n",
    "        \n",
    "# test image load\n",
    "test_list = []\n",
    "for i in TEST_DIR:\n",
    "    for _, file in enumerate(sorted(glob(i + '/*'))):\n",
    "        test_list.append(file)\n",
    "        \n",
    "print('train list:',len(train_list))\n",
    "print('test list:',len(test_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5643a6a",
   "metadata": {},
   "source": [
    "# 데이터 증강 및 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07fc1022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(data_path_list):\n",
    "    \"\"\"\n",
    "    get label form image path\n",
    "    .../label\\\\*.png or .../label\\\\*.jpg\n",
    "    \"\"\"\n",
    "        label_list = []\n",
    "        for path in data_path_list:\n",
    "            label_list.append(path.split('/')[-1].split('\\\\')[-2])\n",
    "        return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f1ef3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_list, classes, transform=None):\n",
    "        self.path_list = data_list\n",
    "        self.label = get_label(data_list)\n",
    "        self.transform = transform\n",
    "        self.classes = classes\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.path_list)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        image = Image.open(self.path_list[index]).convert(\"RGB\")\n",
    "        image = image.resize([100, 100], Image.BILINEAR)\n",
    "        image = np.asarray(image)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, self.classes.index(self.label[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14d70468",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 150\n",
    "learning_rate = 1e-4\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd7481b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class getRandomEraser(object):\n",
    "    \"\"\"\n",
    "    randomly erase patch in images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.p=0.5 # activation probability baseline\n",
    "        self.s_l = 0.02 # s low_value\n",
    "        self.s_h = 0.4 # s high_value\n",
    "        self.r_l = 0.3 # r low_value\n",
    "        self.r_h = 1/0.3 # r high_value\n",
    "        self.v_l = 0 #\n",
    "        self.v_h = 255 #\n",
    "        self.pixel_level=False #\n",
    "\n",
    "    def __call__(self, image, pixel_level=False):\n",
    "        \n",
    "        image = np.asarray(image) # load image\n",
    "        width, height, channels = image.shape[1], image.shape[0], image.shape[2]\n",
    "        p_1 = torch.rand(1) # activate determination\n",
    "        \n",
    "        if p_1 > self.p:\n",
    "            return image\n",
    "        \n",
    "        while True:\n",
    "            s = np.random.uniform(self.s_l, self.s_h) * height * width # patch surface area\n",
    "            r = np.random.uniform(self.r_l, self.r_h) # patch size ratio\n",
    "            w = int(np.sqrt(s / r)) # patch width\n",
    "            h = int(np.sqrt(s * r)) # patch height\n",
    "            left = np.random.randint(0, width) # patch left coordinate\n",
    "            top = np.random.randint(0, height) # patch top coordinate\n",
    "            \n",
    "            if left + w <= width and top + h <= height:\n",
    "                break\n",
    "         \n",
    "        if pixel_level:\n",
    "            c = np.random.uniform(self.v_l, self.v_h, (self.h, self.w, channels))\n",
    "        else:\n",
    "            c = np.random.uniform(self.v_l, self.v_h) # randomly pick value for patch\n",
    "\n",
    "        image[top:top + h, left:left + w, :] = c # replace patch data value\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a0a6e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToPILImage(),\n",
    "     transforms.RandomRotation(degrees=20),\n",
    "     transforms.RandomHorizontalFlip(p=0.5),\n",
    "     getRandomEraser(),\n",
    "     transforms.ToTensor()\n",
    "     ])\n",
    "\n",
    "# 3 classes\n",
    "classes = ('burn_disease', 'healthy', 'leafspot')\n",
    "\n",
    "# train data load\n",
    "train_data = CustomDataset(train_list, classes, transform)\n",
    "\n",
    "# split data into train, valid (0.9:0.1)\n",
    "num_train = int(len(train_data) * 0.9)\n",
    "split_train, split_valid = random_split(train_data, [num_train, len(train_data) - num_train])\n",
    "\n",
    "# test data load\n",
    "test_data = CustomDataset(test_list, classes, transform)\n",
    "\n",
    "# dataloader\n",
    "train_loader = torch.utils.data.DataLoader(split_train,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = True\n",
    "                                          )\n",
    "valid_loader = torch.utils.data.DataLoader(split_valid,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = True\n",
    "                                          )\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fa3013",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e72fb0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.528662383556366, LR: 0.0001\n",
      "Epoch: 2, Loss: 0.4043704569339752, LR: 0.0001\n",
      "Epoch: 3, Loss: 0.48667702078819275, LR: 0.0001\n",
      "Epoch: 4, Loss: 0.6524680852890015, LR: 0.0001\n",
      "Epoch: 5, Loss: 0.40861985087394714, LR: 0.0001\n",
      "Epoch: 6, Loss: 0.33604300022125244, LR: 0.0001\n",
      "Epoch: 7, Loss: 0.3212893307209015, LR: 0.0001\n",
      "Epoch: 8, Loss: 0.0794743075966835, LR: 0.0001\n",
      "Epoch: 9, Loss: 0.024465633556246758, LR: 0.0001\n",
      "Epoch: 10, Loss: 0.9416611194610596, LR: 0.0001\n",
      "Epoch: 11, Loss: 0.8558043837547302, LR: 0.0001\n",
      "Epoch: 12, Loss: 0.21286779642105103, LR: 0.0001\n",
      "Epoch: 13, Loss: 0.3608686923980713, LR: 0.0001\n",
      "Epoch: 14, Loss: 0.19236288964748383, LR: 0.0001\n",
      "Epoch: 15, Loss: 0.24971675872802734, LR: 0.0001\n",
      "Epoch: 16, Loss: 1.5285449028015137, LR: 0.0001\n",
      "Epoch: 17, Loss: 0.23856396973133087, LR: 0.0001\n",
      "Epoch: 18, Loss: 0.14872746169567108, LR: 0.0001\n",
      "Epoch: 19, Loss: 0.10018083453178406, LR: 0.0001\n",
      "Epoch: 20, Loss: 0.9391452670097351, LR: 0.0001\n",
      "Epoch: 21, Loss: 1.4592465162277222, LR: 0.0001\n",
      "Epoch: 22, Loss: 0.14767178893089294, LR: 0.0001\n",
      "Epoch: 23, Loss: 0.35060569643974304, LR: 0.0001\n",
      "Epoch: 24, Loss: 0.0517377108335495, LR: 0.0001\n",
      "Epoch: 25, Loss: 0.32919126749038696, LR: 0.0001\n",
      "Epoch: 26, Loss: 0.021389976143836975, LR: 0.0001\n",
      "Epoch: 27, Loss: 0.4955526888370514, LR: 0.0001\n",
      "Epoch: 28, Loss: 0.7887883186340332, LR: 0.0001\n",
      "Epoch: 29, Loss: 1.0354483127593994, LR: 0.0001\n",
      "Epoch: 30, Loss: 0.3237158954143524, LR: 0.0001\n",
      "Epoch: 31, Loss: 1.1102317571640015, LR: 0.0001\n",
      "Epoch: 32, Loss: 0.35676494240760803, LR: 0.0001\n",
      "Epoch: 33, Loss: 0.20677542686462402, LR: 0.0001\n",
      "Epoch: 34, Loss: 0.06804633885622025, LR: 0.0001\n",
      "Epoch: 35, Loss: 0.38742026686668396, LR: 0.0001\n",
      "Epoch: 36, Loss: 6.639852523803711, LR: 0.0001\n",
      "Epoch: 37, Loss: 0.6387650966644287, LR: 0.0001\n",
      "Epoch: 38, Loss: 0.15467005968093872, LR: 0.0001\n",
      "Epoch: 39, Loss: 0.06424213200807571, LR: 0.0001\n",
      "Epoch: 40, Loss: 0.27032673358917236, LR: 0.0001\n",
      "Epoch: 41, Loss: 0.37432730197906494, LR: 0.0001\n",
      "Epoch: 42, Loss: 0.005596659611910582, LR: 0.0001\n",
      "Epoch: 43, Loss: 5.262666702270508, LR: 0.0001\n",
      "Epoch: 44, Loss: 0.7452404499053955, LR: 0.0001\n",
      "Epoch: 45, Loss: 2.6709725856781006, LR: 0.0001\n",
      "Epoch: 46, Loss: 0.26331627368927, LR: 0.0001\n",
      "Epoch: 47, Loss: 0.5018525719642639, LR: 0.0001\n",
      "Epoch: 48, Loss: 0.9506625533103943, LR: 0.0001\n",
      "Epoch: 49, Loss: 0.3303844630718231, LR: 0.0001\n",
      "Epoch: 50, Loss: 0.1755807250738144, LR: 0.0001\n",
      "Epoch: 51, Loss: 0.011981028132140636, LR: 0.0001\n",
      "Epoch: 52, Loss: 0.015007797628641129, LR: 0.0001\n",
      "Epoch: 53, Loss: 0.05314984917640686, LR: 0.0001\n",
      "Epoch: 54, Loss: 0.04386739060282707, LR: 0.0001\n",
      "Epoch: 55, Loss: 0.046580031514167786, LR: 0.0001\n",
      "Epoch: 56, Loss: 0.1942567378282547, LR: 0.0001\n",
      "Epoch: 57, Loss: 0.5712621808052063, LR: 0.0001\n",
      "Epoch: 58, Loss: 0.13293331861495972, LR: 0.0001\n",
      "Epoch: 59, Loss: 0.5836308598518372, LR: 0.0001\n",
      "Epoch: 60, Loss: 5.597295761108398, LR: 0.0001\n",
      "Epoch: 61, Loss: 0.0726967379450798, LR: 0.0001\n",
      "Epoch: 62, Loss: 0.028196536004543304, LR: 0.0001\n",
      "Epoch: 63, Loss: 0.1772444099187851, LR: 0.0001\n",
      "Epoch: 64, Loss: 0.006467388477176428, LR: 0.0001\n",
      "Epoch: 65, Loss: 1.0704253911972046, LR: 0.0001\n",
      "Epoch: 66, Loss: 0.046166207641363144, LR: 0.0001\n",
      "Epoch: 67, Loss: 0.3007902503013611, LR: 0.0001\n",
      "Epoch: 68, Loss: 0.009628670290112495, LR: 0.0001\n",
      "Epoch: 69, Loss: 0.21349774301052094, LR: 0.0001\n",
      "Epoch: 70, Loss: 0.5660354495048523, LR: 0.0001\n",
      "Epoch: 71, Loss: 0.5402623414993286, LR: 0.0001\n",
      "Epoch: 72, Loss: 0.2932053506374359, LR: 0.0001\n",
      "Epoch: 73, Loss: 2.4194886684417725, LR: 0.0001\n",
      "Epoch: 74, Loss: 0.02792721800506115, LR: 0.0001\n",
      "Epoch: 75, Loss: 0.19987235963344574, LR: 0.0001\n",
      "Epoch: 76, Loss: 0.01304509025067091, LR: 0.0001\n",
      "Epoch: 77, Loss: 0.05721017345786095, LR: 0.0001\n",
      "Epoch: 78, Loss: 0.26604217290878296, LR: 0.0001\n",
      "Epoch: 79, Loss: 0.03651602938771248, LR: 0.0001\n",
      "Epoch: 80, Loss: 0.02329009585082531, LR: 0.0001\n",
      "Epoch: 81, Loss: 0.2235366553068161, LR: 0.0001\n",
      "Epoch: 82, Loss: 0.7072188854217529, LR: 0.0001\n",
      "Epoch: 83, Loss: 0.2639414370059967, LR: 0.0001\n",
      "Epoch: 84, Loss: 1.0281627178192139, LR: 0.0001\n",
      "Epoch: 85, Loss: 0.4097999036312103, LR: 0.0001\n",
      "Epoch: 86, Loss: 0.008405850268900394, LR: 0.0001\n",
      "Epoch: 87, Loss: 0.040076594799757004, LR: 0.0001\n",
      "Epoch: 88, Loss: 0.014329410158097744, LR: 0.0001\n",
      "Epoch: 89, Loss: 0.18382732570171356, LR: 0.0001\n",
      "Epoch: 90, Loss: 0.15635369718074799, LR: 0.0001\n",
      "Epoch: 91, Loss: 0.2838628590106964, LR: 0.0001\n",
      "Epoch: 92, Loss: 0.47034773230552673, LR: 0.0001\n",
      "Epoch: 93, Loss: 1.5857843160629272, LR: 0.0001\n",
      "Epoch: 94, Loss: 3.1307880878448486, LR: 0.0001\n",
      "Epoch: 95, Loss: 0.056733388453722, LR: 0.0001\n",
      "Epoch: 96, Loss: 0.0069336178712546825, LR: 0.0001\n",
      "Epoch: 97, Loss: 0.328011155128479, LR: 0.0001\n",
      "Epoch: 98, Loss: 5.463857650756836, LR: 0.0001\n",
      "Epoch: 99, Loss: 0.6934605240821838, LR: 0.0001\n",
      "Epoch: 100, Loss: 0.05499076843261719, LR: 0.0001\n",
      "Epoch: 101, Loss: 0.7636749148368835, LR: 0.0001\n",
      "Epoch: 102, Loss: 0.024828700348734856, LR: 0.0001\n",
      "Epoch: 103, Loss: 0.014163937419652939, LR: 0.0001\n",
      "Epoch: 104, Loss: 0.19454187154769897, LR: 0.0001\n",
      "Epoch: 105, Loss: 0.6137295961380005, LR: 0.0001\n",
      "Epoch: 106, Loss: 0.8688384890556335, LR: 0.0001\n",
      "Epoch: 107, Loss: 0.01802665926516056, LR: 0.0001\n",
      "Epoch: 108, Loss: 0.7233728766441345, LR: 0.0001\n",
      "Epoch: 109, Loss: 0.03662341460585594, LR: 0.0001\n",
      "Epoch: 110, Loss: 0.006650826428085566, LR: 0.0001\n",
      "Epoch: 111, Loss: 0.9627024531364441, LR: 0.0001\n",
      "Epoch: 112, Loss: 0.13015083968639374, LR: 0.0001\n",
      "Epoch: 113, Loss: 0.5502211451530457, LR: 0.0001\n",
      "Epoch: 114, Loss: 0.1840483546257019, LR: 0.0001\n",
      "Epoch: 115, Loss: 3.403879165649414, LR: 0.0001\n",
      "Epoch: 116, Loss: 0.7981476783752441, LR: 0.0001\n",
      "Epoch: 117, Loss: 0.19630272686481476, LR: 0.0001\n",
      "Epoch: 118, Loss: 0.8479949831962585, LR: 0.0001\n",
      "Epoch: 119, Loss: 2.6260902881622314, LR: 0.0001\n",
      "Epoch: 120, Loss: 0.05539770796895027, LR: 0.0001\n",
      "Epoch: 121, Loss: 0.04987369105219841, LR: 0.0001\n",
      "Epoch: 122, Loss: 0.00992549117654562, LR: 0.0001\n",
      "Epoch: 123, Loss: 0.1643504798412323, LR: 0.0001\n",
      "Epoch: 124, Loss: 0.0335783027112484, LR: 0.0001\n",
      "Epoch: 125, Loss: 0.5348336696624756, LR: 0.0001\n",
      "Epoch: 126, Loss: 0.03677089884877205, LR: 0.0001\n",
      "Epoch: 127, Loss: 1.0770115852355957, LR: 0.0001\n",
      "Epoch: 128, Loss: 0.017409050837159157, LR: 0.0001\n",
      "Epoch: 129, Loss: 0.7851619720458984, LR: 0.0001\n",
      "Epoch: 130, Loss: 0.06029212102293968, LR: 0.0001\n",
      "Epoch: 131, Loss: 0.0031188365537673235, LR: 0.0001\n",
      "Epoch: 132, Loss: 0.09923762083053589, LR: 0.0001\n",
      "Epoch: 133, Loss: 0.006421249359846115, LR: 0.0001\n",
      "Epoch: 134, Loss: 0.053979452699422836, LR: 0.0001\n",
      "Epoch: 135, Loss: 0.0013957017799839377, LR: 0.0001\n",
      "Epoch: 136, Loss: 0.08702630549669266, LR: 0.0001\n",
      "Epoch: 137, Loss: 0.033251482993364334, LR: 0.0001\n",
      "Epoch: 138, Loss: 2.735626220703125, LR: 0.0001\n",
      "Epoch: 139, Loss: 0.25400832295417786, LR: 0.0001\n",
      "Epoch: 140, Loss: 0.10542277246713638, LR: 0.0001\n",
      "Epoch: 141, Loss: 0.04107443615794182, LR: 0.0001\n",
      "Epoch: 142, Loss: 0.7020496726036072, LR: 0.0001\n",
      "Epoch: 143, Loss: 0.3280571401119232, LR: 0.0001\n",
      "Epoch: 144, Loss: 0.005850445944815874, LR: 0.0001\n",
      "Epoch: 145, Loss: 0.5543753504753113, LR: 0.0001\n",
      "Epoch: 146, Loss: 0.37002047896385193, LR: 0.0001\n",
      "Epoch: 147, Loss: 0.002352108946070075, LR: 0.0001\n",
      "Epoch: 148, Loss: 0.32496705651283264, LR: 0.0001\n",
      "Epoch: 149, Loss: 0.03383727744221687, LR: 0.0001\n",
      "Epoch: 150, Loss: 0.8404736518859863, LR: 0.0001\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet50(pretrained=False).to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "loss_func = nn.CrossEntropyLoss().to(device) # Used CE instead of BCE\n",
    "\n",
    "for epochs in range(num_epochs):\n",
    "    \n",
    "    for image, label in train_loader:\n",
    "                \n",
    "        pred = model(image.to(device))\n",
    "        loss = loss_func(pred, label.to(device))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Epoch: {}, Loss: {}, LR: {}'.format(epochs+1, loss.item(), learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4973b69",
   "metadata": {},
   "source": [
    "# Validataion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47e08c3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Valid Data: 93.02325439453125%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for image,label in valid_loader:\n",
    "        \n",
    "        pred = model(image.to(device).float())\n",
    "        loss = loss_func(pred, label.to(device))\n",
    "        \n",
    "        _, output_index = torch.max(pred, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (output_index == label.to(device)).sum().float()\n",
    "    print(\"Accuracy of Valid Data: {}%\".format(100.0*correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430f63fa",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90ae8914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 2, 2, 2, 2, 2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for image, label in test_loader:     \n",
    "\n",
    "        pred = model(image.to(device).float())\n",
    "        loss = loss_func(pred, label.to(device))\n",
    "        \n",
    "        _, output_index = torch.max(pred, 1)\n",
    "        print(output_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cbd6ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "for i in range(len(output_index)):\n",
    "    pred_list.append(int(output_index[i]))\n",
    "    \n",
    "test_path_list = []\n",
    "for i in range(len(test_list)):\n",
    "    test_path_list.append(test_list[i].split('/')[-1])\n",
    "                     \n",
    "df = pd.DataFrame(list(zip(test_path_list, pred_list)), columns =['Name', 'pred'])\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
